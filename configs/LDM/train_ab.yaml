dataset:
  train:
    # Antibody
    - class: DynamicBatchWrapper
      dataset:
        class: AntibodyDataset
        mmap_dir: ./datasets/antibody/SAbDab/processed
        specify_index: ./datasets/antibody/SAbDab/processed/train_index.txt
        length_type: block
      complexity: n*n
      ubound_per_batch: 100000
      n_use_max_in_batch: true
  valid:
    - class: DynamicBatchWrapper
      dataset: 
        class: AntibodyDataset
        mmap_dir: ./datasets/antibody/SAbDab/processed
        specify_index: ./datasets/antibody/SAbDab/processed/valid_index.txt
        length_type: block
      complexity: n*n
      ubound_per_batch: 100000
      n_use_max_in_batch: true

dataloader:
  train:
    shuffle: true
    num_workers: 16
  valid:
    num_workers: 8

trainer:
  class: LDMTrainer
  criterion: loss
  config:
    max_epoch: 500
    save_topk: 10
    val_freq: 1
    save_dir: ./ckpts/PRIME-AB/LDM
    patience: 10
    metric_min_better: true
    proj_name: PRIME-AB # for wandb
    logger: tensorboard # if you want to use wandb, please comment this line 

    optimizer:
      class: AdamW
      lr: 1.0e-4

    scheduler:
      class: ReduceLROnPlateau
      factor: 0.8
      patience: 5
      mode: min
      frequency: val_epoch
      min_lr: 5.0e-6

model:
  class: LDMMolDesign
  autoencoder_ckpt: 'ckpts/PRIME-AB/Encoder/best_model.ckpt' # load by yourself
  latent_deterministic: true  # without resample in training
  hidden_size: 512
  num_steps: 100
  h_loss_weight: 1.0
  std: 10.0
  diffusion_opt:
    trans_seq_type: Diffusion
    trans_pos_type: Diffusion
    encoder_type: EPT
    encoder_opt:
      n_layers: 6
      n_rbf: 64
      cutoff: 3.0  # already normalized
      n_head: 8
      use_edge_feat: true
      pre_norm: true
      efficient: false
      vector_act: layernorm
  is_aa_corrupt_ratio: 0.1

load_ckpt: ''
